{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e1ee6006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from urllib.parse import urlparse, parse_qs, urlunparse\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import re\n",
    "import csv\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "\n",
    "import arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2b120116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful functions\n",
    "\n",
    "def prepare_base_url(full_url):\n",
    "    parsed_url = urlparse(full_url)\n",
    "    query_params = parse_qs(parsed_url.query)\n",
    "    query_params.pop('page', None)\n",
    "\n",
    "    # Reconstruct the query string, ensuring spaces are encoded correctly\n",
    "    query_string = \"&\".join(f\"{key}={value[0].replace(' ', '%20')}\" for key, value in query_params.items())\n",
    "    new_parsed_url = parsed_url._replace(query=query_string)\n",
    "    \n",
    "    return urlunparse(new_parsed_url)\n",
    "\n",
    "def get_total_pages_from_inspire(driver, full_query_url):\n",
    "    # Prepare the base URL by removing the 'page' parameter\n",
    "    base_url = prepare_base_url(full_query_url)\n",
    "    print(base_url)\n",
    "    \n",
    "    # Fetch the first page to get the total number of results\n",
    "    driver.get(base_url + \"&page=1\")\n",
    "    # Scroll to the bottom of the page\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "    # Wait for the page to load\n",
    "    time.sleep(2)  # Adjust this time based on your network speed\n",
    "\n",
    "    html_content = driver.page_source\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Find the total number of results\n",
    "    results_span = soup.find('span', string=re.compile(r'\\d+\\sresults'))\n",
    "    total_results = int(results_span.text.split()[0].replace(',', '')) if results_span else 0\n",
    "    \n",
    "    # Extract the results per page from the URL\n",
    "    match = re.search(r\"&size=(\\d+)&\", full_query_url)\n",
    "    results_per_page = int(match.group(1)) if match else 25  # Default to 25 if not found\n",
    "    \n",
    "    # Calculate the total number of pages\n",
    "    total_pages = math.ceil(total_results / results_per_page)\n",
    "    \n",
    "    return total_pages\n",
    "\n",
    "# Function to extract the number of papers per page\n",
    "def get_papers_per_page(url):\n",
    "    # Regular expression to find the size parameter\n",
    "    match = re.search(r\"&size=(\\d+)&\", url)\n",
    "\n",
    "    return int(match.group(1))\n",
    "\n",
    "# Function to extract DOI\n",
    "def extract_doi(entry):\n",
    "    doi_pattern = re.compile(r'(10\\.\\d{4,9}/[-._;()/:A-Za-z0-9]+)')\n",
    "    doi = None\n",
    "    doi_tag = entry.find('a', href=doi_pattern)\n",
    "    if doi_tag:\n",
    "        doi_match = doi_pattern.search(doi_tag['href'])\n",
    "        doi = doi_match.group(0) if doi_match else None\n",
    "    return doi\n",
    "\n",
    "# Function to extract ePrint number\n",
    "def extract_eprint_number(entry):\n",
    "    eprint_number = None\n",
    "    eprint_section = entry.find(lambda tag: tag.name == 'a' and 'arxiv.org' in tag.get('href', ''))\n",
    "    if eprint_section:\n",
    "        eprint_number = eprint_section.get_text(strip=True)\n",
    "    return eprint_number\n",
    "\n",
    "# Function to extract arXiv category\n",
    "def extract_arxiv_category_v2(entry):\n",
    "    arxiv_category = None\n",
    "    eprint_link = entry.find(lambda tag: tag.name == 'a' and 'arxiv.org' in tag.get('href', ''))\n",
    "    if eprint_link:\n",
    "        category_span = eprint_link.find_next_sibling('span')\n",
    "        if category_span:\n",
    "            arxiv_category = category_span.get_text(strip=True).strip('[]')\n",
    "    return arxiv_category\n",
    "\n",
    "# Revised function to extract citation counts based on the demonstrated algorithm\n",
    "def extract_citation_count_v2(entry):\n",
    "    citation_tag = entry.find_next(lambda tag: tag.name == 'span' and ('citation' in tag.text or 'citations' in tag.text))\n",
    "    if citation_tag:\n",
    "        # Remove commas from the string before converting to int\n",
    "        citation_count = int(citation_tag.text.split()[0].replace(',', ''))\n",
    "        return citation_count\n",
    "    return 0\n",
    "\n",
    "# Function to extract paper details including citation counts\n",
    "def extract_paper_details_v9(entry):\n",
    "    #title_tag = entry.find('span', {'data-test-id': 'literature-detail-title'})\n",
    "    #title = title_tag.get_text(strip=True) if title_tag else None\n",
    "    \n",
    "    title = extract_title_with_latex(entry)\n",
    "    \n",
    "    authors_tags = entry.find_all('a', {'data-test-id': 'author-link'})\n",
    "    authors = ', '.join([tag.get_text(strip=True) for tag in authors_tags])\n",
    "\n",
    "    eprint_number = extract_eprint_number(entry)\n",
    "    arxiv_category = extract_arxiv_category_v2(entry)\n",
    "    doi = extract_doi(entry)\n",
    "    citation_count = extract_citation_count_v2(entry)\n",
    "\n",
    "    return {\n",
    "        'title': title,\n",
    "        'authors': authors,\n",
    "        'eprint_number': eprint_number,\n",
    "        'arXiv_category': arxiv_category,\n",
    "        'doi': doi,\n",
    "        'citation_count': citation_count\n",
    "    }\n",
    "\n",
    "def remove_accents(input_str):\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
    "    only_ascii = nfkd_form.encode('ASCII', 'ignore')\n",
    "    return only_ascii.decode('ASCII')\n",
    "\n",
    "def extract_authors(entry):\n",
    "    authors_tags = entry.find_all('a', {'data-test-id': 'author-link'})\n",
    "    authors_list = [remove_accents(tag.get_text(strip=True)) for tag in authors_tags]\n",
    "    return \", \".join(authors_list)\n",
    "\n",
    "# Function to filter out duplicate and invalid entries\n",
    "def filter_valid_entries(entries):\n",
    "    filtered_entries = []\n",
    "    seen_titles = set()\n",
    "    for entry in entries:\n",
    "        title = entry.find('span', {'data-test-id': 'literature-detail-title'})\n",
    "        authors = entry.find_all('a', {'data-test-id': 'author-link'})\n",
    "        if title and authors and title.get_text(strip=True) not in seen_titles:\n",
    "            seen_titles.add(title.get_text(strip=True))\n",
    "            filtered_entries.append(entry)\n",
    "    return filtered_entries\n",
    "\n",
    "def fetch_abstract(eprint_number):\n",
    "    if not eprint_number:\n",
    "        return \"Abstract not found\"\n",
    "    try:\n",
    "        client = arxiv.Client()\n",
    "        search = arxiv.Search(id_list=[eprint_number])\n",
    "        for result in client.results(search):\n",
    "            return result.summary\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching abstract: {str(e)}\"\n",
    "    return \"Abstract not found\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6394674d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, NavigableString\n",
    "\n",
    "def extract_title_with_latex(entry):\n",
    "    title_element = entry.find('span', {'data-test-id': 'literature-detail-title'})\n",
    "    if not title_element:\n",
    "        return None\n",
    "\n",
    "    def extract_text_latex(el):\n",
    "        text_parts = []\n",
    "        for content in el.children:\n",
    "            if isinstance(content, NavigableString):\n",
    "                current_text = str(content)\n",
    "                if text_parts:\n",
    "                    # Check and adjust space at the interface\n",
    "                    if text_parts[-1].endswith(\" \") and current_text.startswith(\" \"):\n",
    "                        current_text = current_text.lstrip()  # Remove leading space\n",
    "                    elif not text_parts[-1].endswith(\" \") and not current_text.startswith(\" \"):\n",
    "                        current_text = \" \" + current_text  # Add leading space\n",
    "                text_parts.append(current_text)\n",
    "            elif content.name == 'span' and 'katex' in content.get('class', []):\n",
    "                # Extract LaTeX content\n",
    "                latex_annotation = content.find('annotation', encoding='application/x-tex')\n",
    "                if latex_annotation:\n",
    "                    latex_text = latex_annotation.get_text()\n",
    "                    # Check and adjust space at the interface\n",
    "                    if text_parts and not text_parts[-1].endswith(\" \"):\n",
    "                        latex_text = \" \" + latex_text\n",
    "                    if not latex_text.endswith(\" \"):\n",
    "                        latex_text += \" \"\n",
    "                    text_parts.append(latex_text)\n",
    "            elif content.name:\n",
    "                # Recursively process other elements\n",
    "                text_parts.append(extract_text_latex(content))\n",
    "        return ''.join(text_parts)\n",
    "\n",
    "    return extract_text_latex(title_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62cde8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the WebDriver for Safari\n",
    "driver = webdriver.Safari()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "de2ee222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page_data(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Find all entries on the page\n",
    "    entries = soup.find_all(lambda tag: tag.name == 'div' and 'literature' in tag.attrs.get('data-test-id', ''))#soup.find_all('div', {'data-test-id': 'literature-results-item'})\n",
    "    filtered_entries = filter_valid_entries(entries)\n",
    "    \n",
    "    # Lists to store data\n",
    "    titles = []\n",
    "    authors = []\n",
    "    eprint_numbers = []\n",
    "    arxiv_categories = []\n",
    "    dois = []\n",
    "    citation_counts = []\n",
    "    \n",
    "    # Extract details from each entry\n",
    "    for entry in filtered_entries:\n",
    "        \n",
    "        title = extract_title_with_latex(entry)\n",
    "        titles.append(title)\n",
    "\n",
    "        # Extract authors using the new function\n",
    "        authors_names = extract_authors(entry)\n",
    "        authors.append(authors_names)\n",
    "        \n",
    "        # Extract eprint number\n",
    "        eprint_number = extract_eprint_number(entry)\n",
    "        arxiv_category = extract_arxiv_category_v2(entry)\n",
    "        \n",
    "        # Extract DOI\n",
    "        doi = extract_doi(entry)\n",
    "        citation_count = extract_citation_count_v2(entry)\n",
    "\n",
    "        eprint_numbers.append(eprint_number)\n",
    "        arxiv_categories.append(arxiv_category)\n",
    "        dois.append(doi)\n",
    "        citation_counts.append(citation_count)\n",
    "\n",
    "    # Create a DataFrame from the extracted data\n",
    "    data = pd.DataFrame({\n",
    "        'Title': titles,\n",
    "        'Authors': authors,\n",
    "        'ePrint Number': eprint_numbers,\n",
    "        'arXiv Category': arxiv_categories,\n",
    "        'DOI': dois,\n",
    "        'Citation Count': citation_counts\n",
    "    })\n",
    "\n",
    "    return data\n",
    "\n",
    "def scrape_all_pages(driver, base_url, total_pages):\n",
    "    all_data = pd.DataFrame()\n",
    "\n",
    "    for page in range(1, total_pages + 1):\n",
    "        # Replace or add the 'page' parameter in the URL\n",
    "        page_url = re.sub(r\"(page=\\d+)\", f\"page={page}\", base_url)\n",
    "        if \"page=\" not in page_url:\n",
    "            page_url += f\"&page={page}\"\n",
    "            \n",
    "        driver.get(page_url)\n",
    "        \n",
    "        # Scroll to the bottom of the page\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        # Wait for the page to load. Adjust the wait time as needed.\n",
    "        time.sleep(3)  # Example: using a static wait time\n",
    "\n",
    "        html_content = driver.page_source\n",
    "        #print(html_content)\n",
    "        \n",
    "        # Scrape the data from the current page\n",
    "        page_data = scrape_page_data(html_content)\n",
    "\n",
    "        # Concatenate the data from this page to the overall data\n",
    "        all_data = pd.concat([all_data, page_data], ignore_index=True)\n",
    "\n",
    "    return all_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5509c86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL to scrape\n",
    "url = \"https://inspirehep.net/literature?sort=mostrecent&size=250&page=1&q=a%20quevedo&ui-citation-summary=true\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5394649d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://inspirehep.net/literature?sort=mostrecent&size=250&q=a%20quevedo&ui-citation-summary=true\n"
     ]
    }
   ],
   "source": [
    "total_pages = get_total_pages_from_inspire(driver, url)\n",
    "papers_per_page = get_papers_per_page(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0e9c63be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = scrape_all_pages(driver, url, total_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "60e9c9a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Authors</th>\n",
       "      <th>ePrint Number</th>\n",
       "      <th>arXiv Category</th>\n",
       "      <th>DOI</th>\n",
       "      <th>Citation Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Quasi-periodic oscillations in rotating and deformed spacetimes</td>\n",
       "      <td>Kuantay Boshkayev, Talgar Konysbayev, Yergali Kurmanov, Marco Muccino, Hernando Quevedo</td>\n",
       "      <td>2312.03630</td>\n",
       "      <td>astro-ph.HE</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Extended black hole geometrothermodynamics</td>\n",
       "      <td>Hernando Quevedo</td>\n",
       "      <td>2312.01535</td>\n",
       "      <td>gr-qc</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Euclid preparation. TBD. Galaxy power spectrum modelling in real space</td>\n",
       "      <td>A. Pezzotta</td>\n",
       "      <td>2312.00679</td>\n",
       "      <td>astro-ph.CO</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gravitational repulsive effects in 3D regular black holes</td>\n",
       "      <td>Orlando Luongo, Hernando Quevedo, S.N. Sajadi</td>\n",
       "      <td>2311.13264</td>\n",
       "      <td>gr-qc</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Moduli Stabilization in String Theory</td>\n",
       "      <td>Liam McAllister, Fernando Quevedo</td>\n",
       "      <td>2310.20559</td>\n",
       "      <td>hep-th</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                    Title  \\\n",
       "0         Quasi-periodic oscillations in rotating and deformed spacetimes   \n",
       "1                              Extended black hole geometrothermodynamics   \n",
       "2  Euclid preparation. TBD. Galaxy power spectrum modelling in real space   \n",
       "3               Gravitational repulsive effects in 3D regular black holes   \n",
       "4                                   Moduli Stabilization in String Theory   \n",
       "\n",
       "                                                                                   Authors  \\\n",
       "0  Kuantay Boshkayev, Talgar Konysbayev, Yergali Kurmanov, Marco Muccino, Hernando Quevedo   \n",
       "1                                                                         Hernando Quevedo   \n",
       "2                                                                              A. Pezzotta   \n",
       "3                                            Orlando Luongo, Hernando Quevedo, S.N. Sajadi   \n",
       "4                                                        Liam McAllister, Fernando Quevedo   \n",
       "\n",
       "  ePrint Number arXiv Category   DOI  Citation Count  \n",
       "0    2312.03630    astro-ph.HE  None               0  \n",
       "1    2312.01535          gr-qc  None               0  \n",
       "2    2312.00679    astro-ph.CO  None               0  \n",
       "3    2311.13264          gr-qc  None               0  \n",
       "4    2310.20559         hep-th  None               2  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the display option to show full content of the column\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ef657c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df' is your DataFrame and it has a column named 'ePrint Number'\n",
    "df['Abstract'] = df['ePrint Number'].apply(fetch_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203e1a9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffff66b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37c7e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a8750a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddd0148",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
